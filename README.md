# Optimus

In this project for my optimization class, I implemented gradient descent, momentum, adagrad, adam, and accelerated gradient with hyperparameter tuning. I optimized the Booth, Beale, Rosenbrock, and Ackley functions, in each case finding a solution within a Euclidean distance of $< 10^{-10}$ to the analytic solution. Note that answer.py was provided to me and was left unedited, the notebook served as a template where I wrote the optimization algorithms, and the report is completely written by me.
